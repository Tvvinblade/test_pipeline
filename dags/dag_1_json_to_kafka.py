from airflow import DAG
from airflow.decorators import task
from datetime import datetime, timedelta
import json
import os
from kafka import KafkaProducer
import time
import logging

logger = logging.getLogger(__name__)

# –ü–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏ (–í–ù–£–¢–†–ò –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)
DATA_DIR = "/opt/data"

# Kafka –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (Docker Compose —Å–µ—Ç—å)
KAFKA_BROKER = "kafka:29093"

TOPICS = {
    "browser_events.jsonl": "browser_events",
    "device_events.jsonl": "device_events",
    "geo_events.jsonl": "geo_events",
    "location_events.jsonl": "location_events"
}

default_args = {
    'owner': 'data-engineer',
    'depends_on_past': False,
    'email_on_failure': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=1),
}

with DAG(
    dag_id='send_jsonl_to_kafka_taskflow',
    default_args=default_args,
    description='DAG –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ JSONL —Ñ–∞–π–ª–æ–≤ –≤ Kafka',
    schedule_interval=None,
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['kafka', 'jsonl'],
) as dag:

    @task
    def send_file_to_kafka(filename: str, topic: str):
        filepath = os.path.join(DATA_DIR, filename)
        
        if not os.path.exists(filepath):
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
            raise FileNotFoundError(f"File not found: {filepath}")
        
        logger.info(f"üì§ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Kafka: {KAFKA_BROKER}")
        producer = KafkaProducer(
            bootstrap_servers=[KAFKA_BROKER],
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',  # –ñ–¥–∞—Ç—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –æ—Ç –≤—Å–µ—Ö replicas
            retries=3,
            max_in_flight_requests_per_connection=1
        )

        logger.info(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {filename} –≤ —Ç–æ–ø–∏–∫ {topic}")
        count = 0
        errors = 0
        
        try:
            with open(filepath, "r") as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        event = json.loads(line.strip())
                        future = producer.send(topic, value=event)
                        # –ñ–¥–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏
                        record_metadata = future.get(timeout=10)
                        count += 1
                        
                        if line_num % 100 == 0:
                            logger.info(f"  –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {count} —Å–æ–æ–±—â–µ–Ω–∏–π...")
                            
                    except json.JSONDecodeError as e:
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ JSON –≤ —Å—Ç—Ä–æ–∫–µ {line_num}: {str(e)[:50]}")
                        errors += 1
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ —Å—Ç—Ä–æ–∫–µ {line_num}: {str(e)}")
                        errors += 1
        finally:
            # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ–º producer
            producer.flush()
            producer.close()
        
        logger.info(f"‚úÖ {filename}: –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {count} —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ç–æ–ø–∏–∫ {topic} (–æ—à–∏–±–æ–∫: {errors})")
        return f"Successfully sent {count} messages from {filename} to {topic}"

    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
    tasks = [send_file_to_kafka(filename, topic) for filename, topic in TOPICS.items()]